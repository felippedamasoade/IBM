{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNet5g7JV7ruEYxI4dzjkcL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felippedamasoade/NLTK/blob/main/learning_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9W1xBibYQKG"
      },
      "outputs": [],
      "source": [
        "# install library\n",
        "#!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import packges\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czbbY1GoYZ5w",
        "outputId": "d152c9f4-03c3-4c4f-83ce-5bc0bff98be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenização**: Dividir texto em sentenças ou palavras.\n",
        "\n",
        "**Stemming e Lemmatização**: Reduzir palavras para sua raiz ou forma base.\n",
        "\n",
        "**Stopwords**: Palavras comuns (como \"a\", \"o\", \"e\") que geralmente são removidas.\n",
        "\n",
        "**tagging de Partes do Discurso**: Identificar a função gramatical de cada palavra.\n",
        "\n",
        "**Parsing**: Analisar a estrutura gramatical das frases."
      ],
      "metadata": {
        "id": "KVyA-A4yZVFK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E77G2q9dZVcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenização"
      ],
      "metadata": {
        "id": "qJMIpQY-ZmkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "text = \"Hello! How are you doing today? NLTK is a great library for NLP.\"\n",
        "# tokenizar sentenças\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentences:\", sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbW5nSTNZol2",
        "outputId": "491bbb80-26ed-4253-e793-770a6f45ba01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Hello!', 'How are you doing today?', 'NLTK is a great library for NLP.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizar em palavras\n",
        "words = word_tokenize(text)\n",
        "print('words',words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCpQmxnxZtf4",
        "outputId": "a9093ad8-3087-4574-9b86-d14a57ab5e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words ['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'NLTK', 'is', 'a', 'great', 'library', 'for', 'NLP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming e Lemmatização\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "#Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "print(\"stems\",stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrzNREKcaPSW",
        "outputId": "10849205-f827-47cc-e6ac-192282ffd990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stems ['hello', '!', 'how', 'are', 'you', 'do', 'today', '?', 'nltk', 'is', 'a', 'great', 'librari', 'for', 'nlp', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lematização\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "print('lemmas',lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Szsv3rIIaiQP",
        "outputId": "0a8ed6ba-58fd-4d07-888e-97e86bb98cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemmas ['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'NLTK', 'is', 'a', 'great', 'library', 'for', 'NLP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopwords\n",
        "text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Filter Words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(\"Filtered Words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KknT6-kYbaJl",
        "outputId": "d26dea19-68c8-478f-b3f6-4e38dc89c78b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words: ['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tagging\n",
        "from nltk import pos_tag\n",
        "tagged_words = pos_tag(words)\n",
        "print(\"Tagged Words:\", tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVYH28Hvco_M",
        "outputId": "ea42e335-3042-4bff-87d9-5337d34c9ebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagged Words: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), (',', ','), ('showing', 'VBG'), ('off', 'RP'), ('the', 'DT'), ('stop', 'NN'), ('words', 'NNS'), ('filtration', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PROJETO DE FILME\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify.util import accuracy as nltk_accuracy\n",
        "import random"
      ],
      "metadata": {
        "id": "kbWgNbt9c547"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#baixar recursos necessarios\n",
        "nltk.download('movie_reviews')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skBhptv8dVCC",
        "outputId": "7f976c75-4562-45e2-b282-32bf77d3ff1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extrac recursos do corpus\n",
        "def extract_features(words):\n",
        "    return dict([(word, True) for word in words])"
      ],
      "metadata": {
        "id": "MC2eGrMad645"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#carregar o corpus de resenhas de filmes\n",
        "fileids_pos = movie_reviews.fileids('pos')\n",
        "fileids_neg = movie_reviews.fileids('neg')"
      ],
      "metadata": {
        "id": "tiHHqZ4peFup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Rotular os documentos e extrair recursos\n",
        "features_pos = [(extract_features(movie_reviews.words(fileids=[f])), 'Positive') for f in fileids_pos]\n",
        "features_neg = [(extract_features(movie_reviews.words(fileids=[f])), 'Negative') for f in fileids_neg]"
      ],
      "metadata": {
        "id": "XDxA1OhfeOhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir o conjunto de dados em treinamento e teste\n",
        "threshold = 0.8\n",
        "num_pos = int(threshold * len(features_pos))\n",
        "num_neg = int(threshold * len(features_neg))"
      ],
      "metadata": {
        "id": "SY58RgNyeZGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_train = features_pos[:num_pos] + features_neg[:num_neg]\n",
        "features_test = features_pos[num_pos:] + features_neg[num_neg:]"
      ],
      "metadata": {
        "id": "ZC5S3IwGefgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of training datapoints: {len(features_train)}\")\n",
        "print(f\"Number of test datapoints: {len(features_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wReIXTTceg5I",
        "outputId": "8bcdf4fb-408c-41e0-aa4d-6976e4ae0653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training datapoints: 1600\n",
            "Number of test datapoints: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o classificador\n",
        "classifier = NaiveBayesClassifier.train(features_train)"
      ],
      "metadata": {
        "id": "-TToK6etejAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliar o classificador\n",
        "accuracy = nltk_accuracy(classifier, features_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lQBO_N_ekyY",
        "outputId": "4d983cac-96f7-4eaa-ce9f-fe673be915ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 73.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar as 10 principais palavras informativas\n",
        "classifier.show_most_informative_features(10)\n",
        "\n",
        "# Testar o classificador com novos dados\n",
        "input_reviews = [\n",
        "    \"The movie was absolutely wonderful, full of great performances and stunning visuals.\",\n",
        "    \"I didn't like the film. The plot was boring and the acting was mediocre.\"\n",
        "]\n",
        "\n",
        "print(\"\\nPredictions:\")\n",
        "for review in input_reviews:\n",
        "    print(\"\\nReview:\", review)\n",
        "    probdist = classifier.prob_classify(extract_features(review.split()))\n",
        "    pred_sentiment = probdist.max()\n",
        "    print(\"Predicted sentiment:\", pred_sentiment)\n",
        "    print(\"Probability:\", round(probdist.prob(pred_sentiment), 2))"
      ],
      "metadata": {
        "id": "oxXXVhLJel6A",
        "outputId": "9b1622dd-0c98-4c67-fea2-33d63c588288",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "             outstanding = True           Positi : Negati =     13.9 : 1.0\n",
            "               insulting = True           Negati : Positi =     13.7 : 1.0\n",
            "              vulnerable = True           Positi : Negati =     13.0 : 1.0\n",
            "               ludicrous = True           Negati : Positi =     12.6 : 1.0\n",
            "             uninvolving = True           Negati : Positi =     12.3 : 1.0\n",
            "              astounding = True           Positi : Negati =     11.7 : 1.0\n",
            "                  avoids = True           Positi : Negati =     11.7 : 1.0\n",
            "             fascination = True           Positi : Negati =     11.0 : 1.0\n",
            "               affecting = True           Positi : Negati =     10.3 : 1.0\n",
            "               animators = True           Positi : Negati =     10.3 : 1.0\n",
            "\n",
            "Predictions:\n",
            "\n",
            "Review: The movie was absolutely wonderful, full of great performances and stunning visuals.\n",
            "Predicted sentiment: Positive\n",
            "Probability: 0.83\n",
            "\n",
            "Review: I didn't like the film. The plot was boring and the acting was mediocre.\n",
            "Predicted sentiment: Negative\n",
            "Probability: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FKGVCsAKeoBn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}